21.02.20
theoretischer teil:
	generelles vorgehen: meine pattern recognition aufgabe teile ich in 3 phasen ein(wang,patrick S.P.,"Pattern Recognition, Machine Intelligence and biometrics", S.3):
		1. Segmentierung -> die logfiles werden so eingeteilt, dass zum einen für mich irrelevante einträge nicht mehr berücksichtigt werden denn letzen endes stehen in den logfiles die kompletten serveraktivitäten als incoming request | outgong response. das wäre quasi der erste teil der segmentierung. der zweite teil wäre dann die einträge so zusammen zu fassen, dass der erste und letzte eintrag mit identischer sessionid eine session repräsentiert. das nenne ich einfach mal session instanz. in dieser instanz stehen dann eigentlich nur noch zeit,sessionid,userid,incoming|outgoing und url.
		2. feature extraction -> hier würde ich dann schauen, welche widgets für den user aktiviert sind, bzw ob er in den einstellungen war. dementsprechend könnte ich mir z.B. einen vektor definieren, der denotiert, welche widgets aktiv sind/angesprochen wurden etc. nun muss ich noch schauen, was für ein feature diese information ist. eine Idee wäre, dass ich z.b. in einem vektor speicher ob ein widget benutzt wurde oder nicht (0/1) (die idee kommt aus introduction to pattern recognition and machine learning)
		3. classification -> hier muss ich noch bisschen mehr zu recherchieren, aber im letzten schritt möchte ich ja wissen, ob im zeitraum x widget y benutzt wurde. also kann ich einfach die vektoren addieren und wenn meine gesuchte komponente 0 ist weiß ich, dass das widget nicht benutzt wurde. und dann könnte ich in der db nachgucken, ob das widget ursprünglich mal aktiviert war.
			--> stichworte: nearest neighbor, (k-means-) distance?
	
	
ist das im großen und ganzen als clustering zu verstehen, wenn nein: ist clustering die lösung des problems?
zur segmentierung: ich möchte eigentlich die

praktischer teil:
	filebeat schaut die ganze zeit, ob in den logfiles ein neuer eintrag drin ist und schickt das an an logstash. -> darf ich hier die filebeat config von kevin benutzen?
	logstash bekommt einen sessionlog eintrag. per regexp kann ich ab hier schon filtern, welche einträge für mich relevant sind. z.b. werden einträge, die keine sessionid haben nicht gematched und wenn etwas nicht gematched wird, wird es ignoriert. außerdem kann ich in einträgen, die gematched werden mir dann fest legen, welche einträge ich speichern muss und welche nicht. eigentlich brauche ich nur [timestamp][sessionid][userid][url]. ich glaube ich könnte hier sogar auch auf outgoing response verzichten. der parser ist größtenteils schon fertig geschrieben.

was kommt als nächstes:
	angenommen, mein theorie ansatz ist richtig und gut. dann muss ich mir überlegen, wie ich meine session instanzen und die vektoren in elasticsearch rein bekomme. außerdem muss ich mir noch überlegen, wie ich dann von elasticsearch bzw kibana aus ein trigger event an das ifp bzw fip schicke. schließlich muss ich auch noch genauer recherchieren, was ich alles für möglichkeiten für kibana und elasticsearch habe.

26.02.20 (treffen mit prof. conrad)
Literatur (bis jetzt):
	wang,patrick S.P.,"Pattern Recognition, Machine Intelligence and biometrics"
	Narasimha Murty, M. ; Devi, V. Susheela, Introduction to pattern recognition and machine learning
	https://www.cs.unm.edu/~mueen/Papers/LogMine.pdf
	elastic search seite
	kibana seite
	logstash seite
	filebeat seite


zu 2:
	verschiedene repräsentationen (z.b. 0/1 vs. #aufrufe)

zu 3:
	manhatten diszanz (für 0/1)
	euklidische distanz (für numerisch)
	minkowski distanz (verallgemeinerung von den anderen)
	assosiation rules: was tritt im zusammenhang mit was auf?


27.02.20
Sachen, die ich beachten muss:
	- manche widgets haben doch keinen eigenen namen in der url, so wie z.b. transaction by date. das heißt, ich muss mir evtl noch was überlegen, wie ich bestimmte widgets auf bestimmte namen mappe.
	- manche widgets kann ich in den logs nicht sehen, z.b. liquidity widgets. da müsste ich ins ifp was zum loggen eintragen.
	- sollten die widgets eigene felder werden?

05.03.20
	ruby script ist in der ersten version fertig. sollte ich dort aber den algorithmus zum event berechnen in eine funktion auslagern?
	so, wie ich die widgets bisher bestimmt habe, komme ich nicht weit. denn widget laden ist nicht das gleiche wie widget benutzen. also muss ich herausfinden, welche seiten vom widget aus aufgerufen werden, für jedes widget. dann muss ich herausfinden, wie ich ohne widget direkt auch die gleiche seite komme. dann wäre die frage: kann ich unterscheiden, ob diese seite nun mit dem widget aufgerufen wurde oder nicht? wenn nein, kann ich das irgendwie mit association rules abschätzen bzw bestimmen? dann muss ich mir überlegen, wie ich die informationen visualisiere. distanzen berechnen z.b. wäre die linien grafik in kibana, muss ich da noch großartig drauf eingehen, wie kibana das berechnet? welche visualisierungen brauche ich noch? muss ich mir evtl selber eine schreiben und wenn ja, ist es realistisch das zu implementieren?
06.03.20
	segmentierung: alle irrelevanten infos löschen
	feature extraction: könnte sein, dass die feature extraction einfach das zählen ist, also die transformation. könnte aber auch die umwandlung von url strings sein
	classification: mit association rules bestimmen, ob es sich um widget handelt oder nicht. das wird anhand der zählung bestimmt

09.03.20
	fragen für prof conrad:
	1. unser elk wird an den kunden ausgeliefert und sollte von daher so automatisiert ablaufen, wie nur möglich. kommt das noch mit in die arbeit rein? also außer die notwendigen skripte, die ich schreibe?
		-> ja kann ich machen aber nicht zu viel
	2. mir ist aufgefallen, dass ich impliziet test driven development mache. ist das ein aspekt, der relevant ist? ist das überhaupt wirklich so? (weil ich weiß, was ich gemacht habe, wie die daten aussehen und welches ergebnis ich haben möchte)
		-> nicht so wichtig
	3. dass widgets auf dem dashboard sind zählt noch nicht als benutzen. erst ein richtiges klicken zählt. ein klick in das widget bringt einen zu einer anderen seite (je nach widget z.b. mit gewissen filtern). beim klicken kann ich nicht ohne weiteres erkennen, dass ein widget geklickt wurde. darf ich den ifp logger so schreiben, dass diese info geloggt wird?
		3.1 wenn nein: ich glaube, ich kann anhand des anzahl der incoming requests oder am url aufbau erkennen, dass es ein widget ist. kann ich mit assiciation rules bestimmen, ob ein widget geklickt wurde? muss ich die rules davor algorithmisch bestimmen oder darf ich die rules selber bestimmen und anhand selber gewählter werte für z.b. support abschätzen, ob eine seite durch ein widget aufgerufen wurde oder nicht? wäre es hier auch noch relevant, ob ich mir die dashboard config als json hole um zu sehen, welche widgets aktiv sind? weil wenn openpayments nicht in der dashboardconfig ist, brauche ich auch nicht prüfen, ob es benutzt wurde.
			->
		3.2 wenn ja: reicht es, wenn ich bei der dem classification teil sage, dass mich distanzen zwischen den einträgen interessieren und dann auf die lines visualization in kibana referiere? müsste ich dann auf den source code eingehen (macht er wirklich das, was ich will?). reicht diese eine visualisierung oder muss da noch mehr mit rein? funktioniert hier die classifizierung immernoch mit AR?
			->
	ich kanns machen wie ich will, die rules benutze ich in der klassifikation
	4. ich habe verscheidene möglichkeiten ausprobiert um meine daten zu transformieren. kommt das mit rein? also mit der tatsache, dass das feature (noch) nicht implementiert ist?
		-> kann ich erwähnen, aber nicht zu viel.

ergebis nicht in zusammenfassung rein sondern als eigener abschnitt & implementierung umbenennen

12.02.20
	Grundbegriffe association rules auf logs angewendet (knowledge discovery in databases)
		- I = {i1,i2,...} Menge an Items. X <= I ist ein Itemset. In meinem Fall sind die widgetIds die Items
		- D ist eine Menge von Transaktionen T mit T <= I. wenn für eine Menge an Items X gilt X <= T sagen wir T enthält X. Transaktion T ist bei mir eine Session, D sind alles Sessions zusammen (eine Datenbank an Sessions, realisiert durch elasticsearch)
		- Items in den Transaktionen sollten nach Zeit(lexikographisch) sortiert sein. Dann kann man X = (x1,x2,x3,...,xn) mit x1 < x2 < x3 < ... < xn darstellen
		- Anzahl elemente im itemset heißt länge des itemset. ein itemset der länge k heißt k-itemset
		- Support: für eine menge X <= I ist der support von X der Anteil der Transaktionen in D, die X enthalten. Also bedeutet z.b. support(X) = 0.5, dass X in der hälfte aller Transaktionen in D vorkommt. Mit anderen worten: wenn summary in der hälfte aller sessions vorkommt, wäre support(summary)=0.5. Aber es konnte auch sowas sein wie support({summary,createUser})
			-> support(x) = anzahl der transaktionen, die x enthalten / anzahl aller transaktionen
		- eine association rule ist: X => Y (implikation) mit X,Y <= I (teilmengen) und X ^ Y = {}
		- der support einer association rule X => Y ist der support über die vereinigung von X und Y. d.h. supp({X=>Y}) = supp({X,Y}). die vereinigung muss in D sein.
		- die konfidenz c einer AR X=>Y ist die w-keit dafür, dass eine transaktion T alle Items aus Y enthält, wenn T alle items aus X enthält
			-> c(x->y) = anzahl von mengen in X, die y enthalten / anzahl von mengen in D, die x enthalten
	
	was mache ich nun damit?
		1. frequent itemsets bestimmen. frequent itemsets sind alle itemsets, die einen support > minsupport haben (minsupport legt der user fest). naiver ansatz würde einfach den support aller elemente von Potenzmenge(I) berechnen. ist aber schlecht weil laufzeit: 2^n. trick: wenn support für eine 1 elementige menge < minsupport haben auch alle mengen, die dieses element haben einen support < minsupport. glaube ich
		2. aus den itemsets aus 1. werden ARs mit minimaler konfidenz gebildet. wenn X häufig auftritt ergibt jede teilmenge A von x eine regel A => (X-A) mit minimalen support. dann muss man noch prüfen, ob diese regel eine minimale konfidenz hat.

		zu 1. : jede teilmenge eines häufig auftretenden itemsets muss auch häufig auftreten

